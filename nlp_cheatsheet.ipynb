{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "412cb895",
   "metadata": {},
   "source": [
    "# NATURAL LANGUAGE PROCESSING (NLP) ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aad2a7",
   "metadata": {},
   "source": [
    "This cheat sheet provides a beginner-friendly introduction to NLP using PyTorch and the Hugging Face library.\n",
    "It covers fundamental concepts and provides runnable examples for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0a38c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd9a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================\n",
    "# 1. Core NLP Components with PyTorch\n",
    "#==================================\n",
    "print(\"# --- Core NLP Components with PyTorch ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7574c6",
   "metadata": {},
   "source": [
    "--- nn.Embedding: Representing Words as Vectors ---\n",
    "Machine learning models can't understand text directly. We need to convert words into numbers.\n",
    "An Embedding layer is a lookup table that stores a vector for each word in the vocabulary.\n",
    "When you pass an index (representing a word) to the layer, it returns the corresponding vector.\n",
    "These vectors are learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c73948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "# - num_embeddings: The size of the vocabulary (how many unique words).\n",
    "# - embedding_dim: The size of the vector for each word.\n",
    "vocab_size = 1000\n",
    "embedding_dim = 50\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690acaf1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Example: Get the vector for the word at index 10\n",
    "word_index = torch.tensor([10], dtype=torch.long)\n",
    "word_vector = embedding_layer(word_index)\n",
    "print(f\"Example Embedding Layer: {embedding_layer}\")\n",
    "print(f\"Shape of vector for one word: {word_vector.shape}\\n\") # (1, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eb11b1",
   "metadata": {},
   "source": [
    "--- Recurrent Neural Networks (RNNs): Processing Sequences ---\n",
    "RNNs are designed to handle sequential data like text. They have a \"memory\" that allows\n",
    "them to retain information from previous steps in the sequence.\n",
    "- nn.LSTM (Long Short-Term Memory): A popular and powerful type of RNN that can learn\n",
    "  long-range dependencies, avoiding the vanishing gradient problem of simple RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeec98b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "# - input_size: The size of the input features (e.g., embedding_dim).\n",
    "# - hidden_size: The size of the hidden state (the \"memory\").\n",
    "# - num_layers: Number of stacked LSTM layers.\n",
    "# - batch_first=True: Makes the input/output tensors have the batch dimension first (batch, seq, feature).\n",
    "hidden_size = 64\n",
    "lstm_layer = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "print(f\"Example LSTM Layer: {lstm_layer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc933a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================\n",
    "# 2. Complete Example: Text Classification with PyTorch\n",
    "#==================================\n",
    "print(\"\\n# --- A Complete Text Classification Example ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbda6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Prepare the Data ---\n",
    "# For this example, we'll create a small, synthetic dataset.\n",
    "# In a real-world scenario, you would use a library like `torchtext` or Hugging Face `datasets`.\n",
    "texts = [\"this movie is great\", \"i really enjoyed this film\", \"what a waste of time\", \"i did not like this at all\"]\n",
    "labels = [1, 1, 0, 0] # 1 for positive, 0 for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6466d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Text Preprocessing ---\n",
    "# 1. Create a vocabulary: a mapping from words to integer indices.\n",
    "word_to_idx = {}\n",
    "for sentence in texts:\n",
    "    for word in sentence.split():\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "vocab_size = len(word_to_idx)\n",
    "print(f\"Vocabulary: {word_to_idx}\")\n",
    "print(f\"Vocabulary Size: {vocab_size}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe158eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tokenize and encode sentences into integer sequences.\n",
    "sequences = []\n",
    "for sentence in texts:\n",
    "    seq = [word_to_idx[word] for word in sentence.split()]\n",
    "    sequences.append(torch.tensor(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e6c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Pad sequences so they all have the same length.\n",
    "# Models typically require inputs to be of a uniform size.\n",
    "# `pad_sequence` pads with 0s by default.\n",
    "padded_sequences = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "X = padded_sequences\n",
    "y = torch.tensor(labels, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f01adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Padded sequences (X):\\n{X}\")\n",
    "print(f\"Labels (y):\\n{y}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a423a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31eafd9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- Step 2: Define the Text Classification Model ---\n",
    "class SimpleTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(SimpleTextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text shape: (batch_size, seq_length)\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded shape: (batch_size, seq_length, embed_dim)\n",
    "\n",
    "        # We only need the final hidden state of the LSTM\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        # hidden shape: (num_layers, batch_size, hidden_dim)\n",
    "\n",
    "        # Squeeze to remove the num_layers dimension\n",
    "        hidden = hidden.squeeze(0)\n",
    "        # hidden shape: (batch_size, hidden_dim)\n",
    "\n",
    "        output = self.fc(hidden)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a6b24",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "embedding_dim = 32\n",
    "hidden_dim = 16\n",
    "output_dim = 1 # One output neuron for binary classification\n",
    "model = SimpleTextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "print(f\"Model Architecture:\\n{model}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f399670d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# --- Step 3: Define Loss and Optimizer ---\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0708e34a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# --- Step 4: The Training Loop ---\n",
    "print(\"Starting training...\")\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for texts_batch, labels_batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(texts_batch)\n",
    "        loss = criterion(predictions, labels_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7354b6ce",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# --- Step 5: Evaluate / Run Inference ---\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Test with a known positive sentence\n",
    "    test_text = \"this film is great\"\n",
    "    test_seq = torch.tensor([word_to_idx[w] for w in test_text.split()])\n",
    "    prediction = model(test_seq.unsqueeze(0)) # Add batch dimension\n",
    "    print(f\"\\nPrediction for '{test_text}': {prediction.item():.3f} (Closer to 1 is positive)\")\n",
    "\n",
    "    # Test with a known negative sentence\n",
    "    test_text = \"i did not like this\"\n",
    "    test_seq = torch.tensor([word_to_idx[w] for w in test_text.split()])\n",
    "    prediction = model(test_seq.unsqueeze(0))\n",
    "    print(f\"Prediction for '{test_text}': {prediction.item():.3f} (Closer to 0 is negative)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2dc47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================\n",
    "# 3. Introduction to Hugging Face `transformers`\n",
    "#==================================\n",
    "print(\"\\n# --- Introduction to Hugging Face `transformers` ---\")\n",
    "# Hugging Face provides easy access to thousands of pre-trained models (like BERT, GPT)\n",
    "# for a wide variety of NLP tasks. This is the standard approach for most modern NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f68877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dce9972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Using a `pipeline` for a Zero-Shot Task ---\n",
    "# The `pipeline` is the easiest way to use a pre-trained model for inference.\n",
    "# \"Zero-shot\" means the model can classify text into labels it has never seen during its training.\n",
    "print(\"\\n# -- Zero-Shot Classification Pipeline --\")\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "sequence_to_classify = \"The new AI regulations will have a big impact.\"\n",
    "candidate_labels = ['politics', 'business', 'technology', 'sports']\n",
    "result = classifier(sequence_to_classify, candidate_labels)\n",
    "print(f\"Text: '{sequence_to_classify}'\")\n",
    "print(f\"Classification Result: {result['labels'][0]} (Score: {result['scores'][0]:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344538f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Using a `pipeline` for Text Generation ---\n",
    "print(\"\\n# -- Text Generation Pipeline --\")\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "generated_text = generator(\"In a world where AI is king,\", max_length=25, num_return_sequences=1)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
